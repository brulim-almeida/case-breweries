# Airflow - OrquestraÃ§Ã£o do Pipeline

## ğŸ“‹ VisÃ£o Geral

Este diretÃ³rio contÃ©m a orquestraÃ§Ã£o do pipeline de dados das cervejarias usando Apache Airflow 3.0.0 com a TaskFlow API.

## ğŸ—ï¸ Arquitetura da DAG

### Pipeline: `breweries_data_pipeline`

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Bronze Ingestion   â”‚  â†’ IngestÃ£o da API (Open Brewery DB)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Silver Transform    â”‚  â†’ TransformaÃ§Ã£o e curadoria (Delta Lake)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Gold Aggregation   â”‚  â†’ AgregaÃ§Ãµes de negÃ³cio (Delta Lake)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pipeline Validate  â”‚  â†’ ValidaÃ§Ã£o de qualidade e relatÃ³rio
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ Tasks da DAG

### 1. **bronze_ingestion**
- **FunÃ§Ã£o**: Ingere dados da API do Open Brewery DB
- **Output**: JSON bruto particionado por data
- **Retries**: 3
- **Timeout**: 30 minutos

### 2. **silver_transformation**
- **FunÃ§Ã£o**: Transforma e limpa dados Bronze â†’ Silver
- **Output**: Delta Lake com dados normalizados
- **Retries**: 2
- **Timeout**: 45 minutos

### 3. **gold_aggregation**
- **FunÃ§Ã£o**: Cria agregaÃ§Ãµes de negÃ³cio
- **Output**: 6 tabelas Delta Lake com mÃ©tricas
- **Retries**: 2
- **Timeout**: 30 minutos

### 4. **validate_pipeline**
- **FunÃ§Ã£o**: Valida execuÃ§Ã£o e qualidade dos dados
- **Output**: RelatÃ³rio de execuÃ§Ã£o completo
- **Retries**: 1

## âš™ï¸ ConfiguraÃ§Ã£o

### Schedule
```python
schedule_interval='0 2 * * *'  # Diariamente Ã s 2h AM UTC
```

### Default Args
```python
{
    'owner': 'data-engineering',
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=2),
    'email_on_failure': True,
}
```

## ğŸš€ Como Usar

### 1. Validar a DAG (sem Airflow)

```bash
# Validar sintaxe e configuraÃ§Ã£o
python3 airflow/validate_dags.py
```

### 2. Iniciar o Airflow (Docker)

```bash
# Subir o ambiente completo (usar docker-compose da raiz do projeto)
cd /home/brunolima_driva/VSCode/case-breweries
docker-compose up -d

# Verificar status
docker-compose ps

# Acessar UI
# http://localhost:8080
# User: airflow
# Password: airflow
```

### 3. Executar a DAG

**Via UI:**
1. Acesse http://localhost:8080
2. Encontre `breweries_data_pipeline`
3. Toggle ON para ativar
4. Clique em "Trigger DAG" para execuÃ§Ã£o manual

**Via CLI:**
```bash
# Trigger manual
docker exec -it airflow-scheduler airflow dags trigger breweries_data_pipeline

# Listar execuÃ§Ãµes
docker exec -it airflow-scheduler airflow dags list-runs -d breweries_data_pipeline

# Ver logs
docker exec -it airflow-scheduler airflow tasks logs breweries_data_pipeline bronze_ingestion <execution_date>
```

### 4. Monitorar ExecuÃ§Ã£o

**Logs em tempo real:**
```bash
# Todos os containers
docker-compose logs -f

# Scheduler especÃ­fico
docker-compose logs -f scheduler

# Worker especÃ­fico
docker-compose logs -f worker
```

**Verificar status:**
```bash
# Status da DAG
docker exec -it airflow-scheduler airflow dags state breweries_data_pipeline <execution_date>

# Status de uma task
docker exec -it airflow-scheduler airflow tasks state breweries_data_pipeline bronze_ingestion <execution_date>
```

## ğŸ“Š Metadata e XCom

A DAG usa XCom para passar metadata entre tasks:

### Bronze â†’ Silver
```json
{
    "total_records": 9038,
    "pages_processed": 181,
    "ingestion_path": "./lakehouse/bronze/breweries/2026/01/21",
    "status": "success"
}
```

### Silver â†’ Gold
```json
{
    "output_records": 9038,
    "distinct_countries": 58,
    "distinct_types": 7,
    "output_path": "./lakehouse/silver/breweries",
    "status": "success"
}
```

### Gold â†’ Validation
```json
{
    "total_aggregations": 6,
    "aggregation_time": 12.5,
    "aggregations": [...],
    "status": "success"
}
```

## ğŸ”” Callbacks e Alertas

### On Failure
- Registra falha nos logs
- Envia email (se configurado)
- Pode integrar com:
  - Slack
  - PagerDuty
  - Teams
  - Telegram

### On Success
- Registra sucesso nos logs
- Pode enviar notificaÃ§Ãµes de conclusÃ£o

## ğŸ“ˆ MÃ©tricas Monitoradas

### Data Quality
- **Records Ingested**: Total de registros da API
- **Records Transformed**: Registros salvos no Silver
- **Data Loss Rate**: Taxa de perda de dados (%)
- **Aggregations Created**: NÃºmero de tabelas Gold

### Performance
- **Ingestion Time**: Tempo de ingestÃ£o Bronze
- **Transformation Time**: Tempo de transformaÃ§Ã£o Silver
- **Aggregation Time**: Tempo de agregaÃ§Ã£o Gold
- **Total Pipeline Time**: Tempo total de execuÃ§Ã£o

### Quality Thresholds
- âš ï¸ **Data Loss > 5%**: Alerta de qualidade
- âš ï¸ **Zero Aggregations**: Falha crÃ­tica
- âš ï¸ **Execution > 2h**: Timeout warning

## ğŸ”§ Troubleshooting

### DAG nÃ£o aparece na UI

```bash
# Verificar logs do scheduler
docker-compose logs scheduler | grep ERROR

# Validar sintaxe
python3 airflow/validate_dags.py

# Listar DAGs reconhecidas
docker exec -it airflow-scheduler airflow dags list
```

### Task falhando

```bash
# Ver logs da task
docker exec -it airflow-scheduler airflow tasks logs \
    breweries_data_pipeline <task_id> <execution_date>

# Testar task localmente
docker exec -it airflow-scheduler airflow tasks test \
    breweries_data_pipeline <task_id> <execution_date>
```

### Problemas de import

```bash
# Verificar Python path
docker exec -it airflow-scheduler python -c "import sys; print('\n'.join(sys.path))"

# Testar imports
docker exec -it airflow-scheduler python -c "from src.layers import BronzeLayer"
```

### ConexÃ£o com banco de dados

```bash
# Verificar conexÃµes
docker exec -it airflow-scheduler airflow connections list

# Resetar DB
docker-compose down -v
docker-compose up -d
```

## ğŸ“ Estrutura de Arquivos

```
/                                      # Raiz do projeto
â”œâ”€â”€ docker-compose.yaml                # â­ ConfiguraÃ§Ã£o Docker principal
â”œâ”€â”€ Dockerfile                         # Build customizado com Delta Lake
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ breweries_pipeline_dag.py     # DAG principal
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/                          # DAGs de desenvolvimento
â”‚   â”œâ”€â”€ logs/                          # Logs das execuÃ§Ãµes
â”‚   â”œâ”€â”€ validate_dags.py               # Script de validaÃ§Ã£o
â”‚   â”œâ”€â”€ test_pipeline.py               # Teste local do pipeline
â”‚   â””â”€â”€ README.md                      # Esta documentaÃ§Ã£o
â”œâ”€â”€ src/                               # CÃ³digo fonte do projeto
â””â”€â”€ lakehouse/                         # Data Lake (Bronze/Silver/Gold)
```

## ğŸ“ PrÃ³ximos Passos

1. **Sensors**: Adicionar FileSensor para verificar dados
2. **Data Quality**: Implementar Great Expectations
3. **Alertas**: Configurar Slack/email notifications
4. **Backfill**: EstratÃ©gia para reprocessamento histÃ³rico
5. **SLA**: Definir SLAs para cada task
6. **Variables**: Usar Airflow Variables para configuraÃ§Ãµes
7. **Connections**: Configurar conexÃµes externas (se necessÃ¡rio)

## ğŸ“š DocumentaÃ§Ã£o Adicional

- [Airflow TaskFlow API](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html)
- [DAG Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)
- [XCom Documentation](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/xcoms.html)
